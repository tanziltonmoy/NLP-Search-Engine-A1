{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import json\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Compare Skip-gram, Skip-gram negative sampling, GloVe models on training loss, training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The models underwent training using a handpicked subset of the Reuters corpus obtained from NLTK. This subset comprises 1000 passages out of the total 54716 available, encompassing 4152 tokens out of a grand total of 1728932.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Training Loss\n",
    "| Model                          | Average Training Loss |\n",
    "|--------------------------------|-----------------------|\n",
    "| Skip-gram                      | 8.051740          |\n",
    "| Skip-gram with Negative Sampling | 1.905731             |\n",
    "| GloVe                        | 1.391059 |\n",
    "\n",
    "\n",
    "\n",
    "#### Training Time\n",
    "\n",
    "| Model                          | Total Training Time |\n",
    "|--------------------------------|---------------------|\n",
    "| Skip-gram                      | 30m 36s            |\n",
    "| Skip-gram with Negative Sampling | 29m 22s            |\n",
    "| GloVe Scratch                    | 3m 12s              |\n",
    "\n",
    "\n",
    "The training data and outcomes indicate the GloVe model achieved the lowest average training loss and shortest training time, showcasing its efficiency and effectiveness. Conversely, the Skip-gram model, despite its longer training duration, had the highest loss, suggesting it might be less efficient in this context. The Skip-gram with Negative Sampling offered a balance, with significantly reduced loss compared to Skip-gram and marginally faster training, highlighting its improved efficiency over the standard Skip-gram model. These points underscore the differences in model performance and training efficiency within the same dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Use Word analogies dataset to calucalte between syntactic and semantic accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_analogy_tasks(file_path):\n",
    "    \"\"\"\n",
    "    Extracts analogy tasks from a specified file, focusing on specific categories.\n",
    "    Categories processed include 'capital-common-countries' and 'gram7-past-tense'.\n",
    "    Stops processing upon reaching the 'gram8-plural' category.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: The path to the file containing analogy tasks.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple of two lists: one for 'capital-common-countries' tasks and another for 'gram7-past-tense' tasks.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Initialize lists for specific analogy types\n",
    "    capital_common_countries = []\n",
    "    gram7_past_tense = []\n",
    "\n",
    "    # Variable to track the current category of analogies being processed\n",
    "    current_section = None\n",
    "\n",
    "    for line in lines[1:]:  # Skip the first line assuming it's a header or format descriptor\n",
    "        # Check and update the current section based on the line content\n",
    "        if ': capital-common-countries' in line:\n",
    "            current_section = capital_common_countries\n",
    "            continue\n",
    "        elif ': gram7-past-tense' in line:\n",
    "            current_section = gram7_past_tense\n",
    "            continue\n",
    "        elif ': gram8-plural' in line:\n",
    "            break  # Exit the loop upon reaching this category\n",
    "\n",
    "        # If the line belongs to a current section, process and add it to the appropriate list\n",
    "        if current_section is not None:\n",
    "            words = line.strip().split()\n",
    "            if len(words) == 4:  # Ensure the line has exactly four words, as expected for analogy tasks\n",
    "                current_section.append(tuple(words))\n",
    "\n",
    "    return capital_common_countries, gram7_past_tense\n",
    "\n",
    "# Load the data\n",
    "file_path = 'word-test.v1.txt'\n",
    "capital_common_countries, past_tense = extract_analogy_tasks(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('Athens', 'Greece', 'Baghdad', 'Iraq'),\n",
       " ('Ukraine', 'Ukrainian', 'Switzerland', 'Swiss'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capital_common_countries[0], capital_common_countries[len(capital_common_countries)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('dancing', 'danced', 'decreasing', 'decreased'),\n",
       " ('writing', 'wrote', 'walking', 'walked'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_tense[0], past_tense[len(past_tense)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_accuracy_skipgram(model, dataset, word2index, index2word):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for word1, word1_target, word2, word2_target in dataset:\n",
    "        # Skip analogy if any word is not in the model's vocabulary to avoid errors\n",
    "        if all(word in word2index for word in [word1, word1_target, word2, word2_target]):\n",
    "            total += 1\n",
    "\n",
    "            # Convert words to indices for model processing\n",
    "            word1_idx = word2index[word1]\n",
    "            word1_target_idx = word2index[word1_target]\n",
    "            word2_idx = word2index[word2]\n",
    "\n",
    "            # Obtain embeddings by averaging center and context (outside) embeddings\n",
    "            word1_emb = (model.embedding_center(torch.tensor([word1_idx], dtype=torch.long)).squeeze(0) +\n",
    "                         model.embedding_outside(torch.tensor([word1_idx], dtype=torch.long)).squeeze(0)) / 2\n",
    "            word1_target_emb = (model.embedding_center(torch.tensor([word1_target_idx], dtype=torch.long)).squeeze(0) +\n",
    "                                model.embedding_outside(torch.tensor([word1_target_idx], dtype=torch.long)).squeeze(0)) / 2\n",
    "            word2_emb = (model.embedding_center(torch.tensor([word2_idx], dtype=torch.long)).squeeze(0) +\n",
    "                         model.embedding_outside(torch.tensor([word2_idx], dtype=torch.long)).squeeze(0)) / 2\n",
    "\n",
    "            # Vector arithmetic to predict the target word's embedding\n",
    "            expected_emb = word2_emb - word1_emb + word1_target_emb\n",
    "\n",
    "            # Compute cosine similarity between expected embedding and all others\n",
    "            similarities = F.cosine_similarity((model.embedding_center.weight + model.embedding_outside.weight) / 2, \n",
    "                                               expected_emb.unsqueeze(0), dim=1)\n",
    "\n",
    "\n",
    "            # Before the loop calculating similarities, define indices_to_exclude\n",
    "            indices_to_exclude = [word2index[word] for word in [word1, word1_target, word2] if word in word2index]\n",
    "            for idx in indices_to_exclude:\n",
    "                similarities[idx] = -1\n",
    "\n",
    "            # Identify the most similar embedding as the predicted word\n",
    "            max_similarity_idx = torch.argmax(similarities).item()\n",
    "\n",
    "            # If the predicted word matches the target, count it as correct\n",
    "            if index2word[str(max_similarity_idx)] == word2_target:\n",
    "                correct += 1\n",
    "\n",
    "    # Calculate and return the overall accuracy\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy_GloVe(model, dataset, word2index, index2word):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for word1, word1_target, word2, word2_target in dataset:\n",
    "        # Ensure all words are in the model's vocabulary to avoid processing OOV words\n",
    "        if all(word in word2index for word in [word1, word1_target, word2, word2_target]):\n",
    "            total += 1\n",
    "\n",
    "            # Retrieve indices for each word to access embeddings\n",
    "            word1_idx, word1_target_idx, word2_idx = [word2index[word] for word in [word1, word1_target, word2]]\n",
    "\n",
    "            # Average center and outside embeddings to get a comprehensive word representation\n",
    "            word1_emb, word1_target_emb, word2_emb = [\n",
    "                (model.center_embedding(torch.tensor([idx], dtype=torch.long)).squeeze(0) +\n",
    "                 model.outside_embedding(torch.tensor([idx], dtype=torch.long)).squeeze(0)) / 2\n",
    "                for idx in [word1_idx, word1_target_idx, word2_idx]\n",
    "            ]\n",
    "\n",
    "            # Vector arithmetic to predict the embedding of the target word\n",
    "            expected_emb = word2_emb - word1_emb + word1_target_emb\n",
    "\n",
    "            # Compute cosine similarities between the predicted embedding and all vocabulary embeddings\n",
    "            similarities = F.cosine_similarity((model.center_embedding.weight + model.outside_embedding.weight) / 2,\n",
    "                                               expected_emb.unsqueeze(0), dim=1)\n",
    "\n",
    "            # Exclude the analogy words from the similarity search to ensure fairness\n",
    "            indices_to_exclude = [word2index[word] for word in [word1, word1_target, word2] if word in word2index]\n",
    "            for idx in indices_to_exclude:\n",
    "                similarities[idx] = -1  # Set excluded indices to a low similarity\n",
    "\n",
    "            # Identify the word most similar to the calculated embedding\n",
    "            max_similarity_idx = torch.argmax(similarities).item()\n",
    "\n",
    "            # Check if the most similar word matches the target word\n",
    "            if index2word[str(max_similarity_idx)] == word2_target:\n",
    "                correct += 1  # Increment correct count if prediction is accurate\n",
    "\n",
    "    # Calculate and return the model's accuracy on the dataset\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy_GloVe_gensim(model, dataset):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for word1, word1_target, word2, word2_target in dataset:\n",
    "        # Check if all words are in the model's vocabulary, skip the analogy if any word is OOV\n",
    "        if all(word in model.key_to_index for word in [word1, word1_target, word2, word2_target]):\n",
    "            total += 1\n",
    "\n",
    "            # Get the embeddings for each word\n",
    "            word1_emb = model[word1]\n",
    "            word1_target_emb = model[word1_target]\n",
    "            word2_emb = model[word2]\n",
    "\n",
    "            # Compute the expected embedding for the target word\n",
    "            expected_emb = word2_emb - word1_emb + word1_target_emb\n",
    "\n",
    "            # Calculate similarities between the expected embedding and all word embeddings in the vocabulary\n",
    "            all_embeddings = model.vectors\n",
    "            similarities = np.dot(all_embeddings, expected_emb) / (np.linalg.norm(all_embeddings, axis=1) * np.linalg.norm(expected_emb))\n",
    "\n",
    "            # Exclude original words from consideration\n",
    "            for word in [word1, word1_target, word2]:\n",
    "                if word in model.key_to_index:\n",
    "                    similarities[model.key_to_index[word]] = -1\n",
    "\n",
    "            max_similarity_idx = np.argmax(similarities)\n",
    "\n",
    "            # Check if the word with the maximum similarity is the target word\n",
    "            if model.index_to_key[max_similarity_idx] == word2_target:\n",
    "                correct += 1\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Skipgram model class\n",
    "class Skipgram(nn.Module):\n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "        # Embedding layers for center and outside words\n",
    "        self.embedding_center = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "    \n",
    "    def forward(self, center, outside, all_vocabs):\n",
    "        # Obtain embeddings for center, outside, and all vocabulary words\n",
    "        center_embedding = self.embedding_center(center)\n",
    "        outside_embedding = self.embedding_outside(outside)\n",
    "        all_vocabs_embedding = self.embedding_outside(all_vocabs)\n",
    "        \n",
    "        # Calculate top and lower terms for loss computation\n",
    "        top_term = torch.exp(outside_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2))\n",
    "        lower_term = all_vocabs_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2)\n",
    "        lower_term_sum = torch.sum(torch.exp(lower_term), 1)\n",
    "        \n",
    "        # Calculate and return loss\n",
    "        loss = -torch.mean(torch.log(top_term / lower_term_sum))\n",
    "        return loss\n",
    "\n",
    "word2index_path = './model/word2index_skipgram.json'  \n",
    "index2word_path = './model/index2word_skipgram.json' \n",
    "model_path = './model/word2vec_model_skipgram.pth'\n",
    "config_path = './model/word2vec_config_skipgram.json'        \n",
    "with open(config_path, 'r') as config_file:\n",
    "    config_skipgram = json.load(config_file)\n",
    "\n",
    "# Initialize model with loaded configuration\n",
    "loaded_model_Skipgram = Skipgram(voc_size=config_skipgram['voc_size'], emb_size=config_skipgram['emb_size'])\n",
    "\n",
    "# Load model state\n",
    "loaded_model_Skipgram.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "loaded_model_Skipgram.eval()  \n",
    "\n",
    "with open(word2index_path, 'r') as file:\n",
    "    word2index_skipgram = json.load(file)\n",
    "\n",
    "with open(index2word_path, 'r') as file:\n",
    "    index2word_skipgram = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipgram Semantic Accuracy: 0.00%\n",
      "Skipgram Syntactic Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the semantic accuracy on the 'capital-common-countries' analogy dataset\n",
    "semantic_accuracy_Skipgram = calculate_accuracy_skipgram(loaded_model_Skipgram, capital_common_countries, word2index_skipgram, index2word_skipgram)\n",
    "\n",
    "# Calculate the syntactic accuracy on the 'gram7-past-tense' analogy dataset\n",
    "syntactic_accuracy_Skipgram = calculate_accuracy_skipgram(loaded_model_Skipgram, past_tense, word2index_skipgram, index2word_skipgram)\n",
    "\n",
    "# Print the semantic accuracy as a percentage\n",
    "print(f\"Skipgram Semantic Accuracy: {semantic_accuracy_Skipgram * 100:.2f}%\")\n",
    "\n",
    "# Print the syntactic accuracy as a percentage\n",
    "print(f\"Skipgram Syntactic Accuracy: {syntactic_accuracy_Skipgram * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "class SkipgramNeg(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(SkipgramNeg, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "        self.logsigmoid        = nn.LogSigmoid()\n",
    "    \n",
    "    def forward(self, center, outside, negative):\n",
    "        #center, outside:  (bs, 1)\n",
    "        #negative       :  (bs, k)\n",
    "        \n",
    "        center_embed   = self.embedding_center(center) #(bs, 1, emb_size)\n",
    "        outside_embed  = self.embedding_outside(outside) #(bs, 1, emb_size)\n",
    "        negative_embed = self.embedding_outside(negative) #(bs, k, emb_size)\n",
    "        \n",
    "        uovc           = outside_embed.bmm(center_embed.transpose(1, 2)).squeeze(2) #(bs, 1)\n",
    "        ukvc           = -negative_embed.bmm(center_embed.transpose(1, 2)).squeeze(2) #(bs, k)\n",
    "        ukvc_sum       = torch.sum(ukvc, 1).reshape(-1, 1) #(bs, 1)\n",
    "        \n",
    "        loss           = self.logsigmoid(uovc) + self.logsigmoid(ukvc_sum)\n",
    "        \n",
    "        return -torch.mean(loss)\n",
    "\n",
    "\n",
    "word2index_path = './model/word2index_skipgram_neg.json'  \n",
    "index2word_path = './model/index2word_skipgram_neg.json' \n",
    "model_path = './model/word2vec_model_skipgram_neg.pth'\n",
    "config_path = './model/word2vec_config_skipgram_neg.json'\n",
    "\n",
    "with open(word2index_path, 'r') as file:\n",
    "    word2index_SkipgramNeg = json.load(file)  # Load the word2index dictionary from the JSON file\n",
    "\n",
    "with open(index2word_path, 'r') as file:\n",
    "    index2word_SkipgramNeg = json.load(file)\n",
    "\n",
    "# Load the model's configuration from a JSON file\n",
    "with open(config_path, 'r') as config_file:\n",
    "    config_SkipgramNeg = json.load(config_file)\n",
    "\n",
    "# Retrieve the configuration values\n",
    "voc_size = config_SkipgramNeg['voc_size']  # Vocabulary size\n",
    "emb_size = config_SkipgramNeg['emb_size']  # Embedding size\n",
    "\n",
    "# Initialize a new Word2Vec model with the loaded configuration\n",
    "loaded_model_SkipgramNeg = SkipgramNeg(voc_size, emb_size)\n",
    "\n",
    "# Load the state dictionary (model parameters) into the initialized model\n",
    "loaded_model_SkipgramNeg.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Set the model to evaluation mode (useful for inference)\n",
    "loaded_model_SkipgramNeg.eval()\n",
    "\n",
    "# Confirm successful model loading\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipgram-neg Semantic Accuracy: 0.00%\n",
      "Skipgram-neg Syntactic Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Calculate semantic accuracy on 'capital-common-countries' analogies\n",
    "semantic_accuracy_Skipgram_neg = calculate_accuracy_skipgram(loaded_model_SkipgramNeg, capital_common_countries, word2index_SkipgramNeg, index2word_SkipgramNeg)\n",
    "# Calculate syntactic accuracy on 'gram7-past-tense' analogies\n",
    "syntactic_accuracy_Skipgram_neg = calculate_accuracy_skipgram(loaded_model_SkipgramNeg, past_tense, word2index_SkipgramNeg, index2word_SkipgramNeg)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Skipgram-neg Semantic Accuracy: {semantic_accuracy_Skipgram_neg * 100:.2f}%\")\n",
    "print(f\"Skipgram-neg Syntactic Accuracy: {syntactic_accuracy_Skipgram_neg * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "class Glove(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Glove, self).__init__()\n",
    "        # Embeddings for center words\n",
    "        self.center_embedding  = nn.Embedding(voc_size, emb_size)\n",
    "        # Embeddings for context (outside) words\n",
    "        self.outside_embedding = nn.Embedding(voc_size, emb_size)\n",
    "        \n",
    "        # Bias terms for center words\n",
    "        self.center_bias       = nn.Embedding(voc_size, 1) \n",
    "        # Bias terms for context (outside) words\n",
    "        self.outside_bias      = nn.Embedding(voc_size, 1)\n",
    "    \n",
    "    def forward(self, center, outside, coocs, weighting):\n",
    "        # Retrieve the embeddings for the center words\n",
    "        center_embeds  = self.center_embedding(center)  # (batch_size, 1, emb_size)\n",
    "        # Retrieve the embeddings for the outside words\n",
    "        outside_embeds = self.outside_embedding(outside)  # (batch_size, 1, emb_size)\n",
    "        \n",
    "        # Retrieve and squeeze the bias for the center words\n",
    "        center_bias    = self.center_bias(center).squeeze(1)\n",
    "        # Retrieve and squeeze the bias for the outside words\n",
    "        target_bias    = self.outside_bias(outside).squeeze(1)\n",
    "        \n",
    "        # Compute the dot product of center and outside word embeddings\n",
    "        inner_product  = outside_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        \n",
    "        # Compute the GloVe loss as the weighted squared error between\n",
    "        # the log co-occurrence counts and the model predictions (dot product + biases)\n",
    "        loss = weighting * torch.pow(inner_product + center_bias + target_bias - coocs, 2)\n",
    "        \n",
    "        # Return the sum of the losses for the batch\n",
    "        return torch.sum(loss)\n",
    "    \n",
    "word2index_path = './model/word2index_glove.json'  \n",
    "index2word_path = './model/index2word_glove.json' \n",
    "model_path = './model/word2vec_model_glove.pth'\n",
    "config_path = './model/word2vec_config_glove.json'\n",
    "with open(word2index_path, 'r') as file:\n",
    "    word2index_Glove = json.load(file)  # Load the word2index dictionary from the JSON file\n",
    "\n",
    "with open(index2word_path, 'r') as file:\n",
    "    index2word_Glove = json.load(file)  # Load the index2word dictionary from the JSON file\n",
    "# Load the model's configuration from a JSON file\n",
    "with open(config_path, 'r') as config_file:\n",
    "    config_Glove = json.load(config_file)\n",
    "\n",
    "# Retrieve the configuration values\n",
    "voc_size = config_Glove['voc_size']  # Vocabulary size\n",
    "emb_size = config_Glove['emb_size']  # Embedding size\n",
    "\n",
    "# Initialize a new Word2Vec model with the loaded configuration\n",
    "loaded_model_Glove = Glove(voc_size, emb_size)\n",
    "\n",
    "# Load the state dictionary (model parameters) into the initialized model\n",
    "loaded_model_Glove.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Set the model to evaluation mode (useful for inference)\n",
    "loaded_model_Glove.eval()\n",
    "\n",
    "# Confirm successful model loading\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe Semantic Accuracy: 0.00%\n",
      "GloVe Syntactic Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "semantic_accuracy_GloVe = calculate_accuracy_GloVe(loaded_model_Glove, capital_common_countries, word2index_Glove, index2word_Glove)\n",
    "syntactic_accuracy_GloVe = calculate_accuracy_GloVe(loaded_model_Glove, past_tense, word2index_Glove, index2word_Glove)\n",
    "print(f\"GloVe Semantic Accuracy: {semantic_accuracy_GloVe * 100:.2f}%\")\n",
    "print(f\"GloVe Syntactic Accuracy: {syntactic_accuracy_GloVe * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.11.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (6.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_10648\\43224521.py:11: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_file_path, word2vec_output_file)\n"
     ]
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Path to the GloVe file (Replace this with your GloVe file path)\n",
    "glove_file_path = 'dataset/glove.6B.100d.txt'  # Example path\n",
    "\n",
    "# File path for the output Word2Vec format file\n",
    "word2vec_output_file = glove_file_path + '.word2vec'\n",
    "\n",
    "# Convert the GloVe file format to the Word2Vec file format\n",
    "glove2word2vec(glove_file_path, word2vec_output_file)\n",
    "\n",
    "# Load the model from the converted Word2Vec format file\n",
    "loaded_model_Glove_Gen = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe gensim Semantic Accuracy: 54.97%\n",
      "GloVe gensim Syntactic Accuracy: 53.40%\n"
     ]
    }
   ],
   "source": [
    "semantic_accuracy_GloVe_Gen = calculate_accuracy_GloVe_gensim(loaded_model_Glove_Gen, capital_common_countries)\n",
    "syntactic_accuracy_GloVe_Gen = calculate_accuracy_GloVe_gensim(loaded_model_Glove_Gen, past_tense)\n",
    "\n",
    "# Print the semantic and syntactic accuracies\n",
    "print(f\"GloVe gensim Semantic Accuracy: {semantic_accuracy_GloVe_Gen * 100:.2f}%\")\n",
    "print(f\"GloVe gensim Syntactic Accuracy: {syntactic_accuracy_GloVe_Gen * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model's Comparison Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model                             | Window Size | Training Loss(taken from Traning Notebooks) | Syntactic Accuracy | Semantic Accuracy |\n",
    "|-----------------------------------|-------------|---------------|--------------------|-------------------|\n",
    "| Skip-gram                         | 2           | 8.051740       | 0.00%              | 0.00%             |\n",
    "| Skip-gram with Negative Sampling  | 2           | 1.905731        | 0.00%              | 0.00%             |\n",
    "| GloVe Scratch                    | 2           |  1.391059        | 0.00%              | 0.00%             |\n",
    "| GloVe (Pre-trained Gensim)        | N/A         | N/A           | 53.40%             | 54.97%            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The table summarizes the performance of different word embedding models, highlighting their window size, training loss, syntactic accuracy, and semantic accuracy. Notably, the Skip-gram and Skip-gram with Negative Sampling models show a training loss of 8.051740 and 1.905731, respectively, but both yield 0% in syntactic and semantic accuracies. Conversely, the GloVe model developed from scratch presents a lower training loss of 1.391059, also with 0% accuracies. The pre-trained GloVe model from Gensim, not constrained by window size and without a specified training loss, significantly outperforms the others with over 53% in both syntactic and semantic accuracies, showcasing the effectiveness of pre-trained embeddings in capturing word relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Use the similarity dataset to find the correlation between your models’ dot product and the provided similarity metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        word1  word2  human_score\n",
      "0       tiger    cat         7.35\n",
      "1       tiger  tiger        10.00\n",
      "2       plane    car         5.77\n",
      "3       train    car         6.31\n",
      "4  television  radio         6.77\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = 'wordsim_similarity_goldstandard.txt'\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(path, sep='\\t', names=['word1', 'word2', 'human_score'])\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity_skipgram_models(model, word_pairs, word2index):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between pairs of words using embeddings from Skipgram or Skipgram-Negative Sampling models.\n",
    "    \n",
    "    Args:\n",
    "    - model: The trained Skipgram or Skipgram-Negative Sampling model.\n",
    "    - word_pairs: A list of tuples containing word pairs for similarity computation.\n",
    "    - word2index: A dictionary mapping words to their respective indices in the model's vocabulary.\n",
    "    \n",
    "    Returns:\n",
    "    - A list of cosine similarities for each word pair. Returns None for pairs containing out-of-vocabulary words.\n",
    "    \"\"\"\n",
    "    similarities = []\n",
    "    for word1, word2 in word_pairs:\n",
    "        # Verify both words are in the model's vocabulary\n",
    "        if word1 in word2index and word2 in word2index:\n",
    "            # Retrieve indices and embeddings for both words\n",
    "            word1_idx, word2_idx = word2index[word1], word2index[word2]\n",
    "            word1_emb, word2_emb = [\n",
    "                (model.embedding_center(torch.tensor([idx], dtype=torch.long)).squeeze(0) +\n",
    "                 model.embedding_outside(torch.tensor([idx], dtype=torch.long)).squeeze(0)) / 2\n",
    "                for idx in [word1_idx, word2_idx]\n",
    "            ]\n",
    "            \n",
    "            # Compute cosine similarity\n",
    "            similarity = 1 - cosine(word1_emb.detach().numpy(), word2_emb.detach().numpy())\n",
    "            similarities.append(similarity)\n",
    "        else:\n",
    "            # Handle out-of-vocabulary words\n",
    "            similarities.append(None)\n",
    "            \n",
    "    return similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity_glove(model, word_pairs, word2index):\n",
    "    \"\"\"\n",
    "    Calculates cosine similarities between pairs of words using GloVe model embeddings.\n",
    "    \n",
    "    Args:\n",
    "    - model: The GloVe model trained from scratch.\n",
    "    - word_pairs: A list of tuples with word pairs for which to compute similarities.\n",
    "    - word2index: Dictionary mapping words to their indices in the model's vocabulary.\n",
    "    \n",
    "    Returns:\n",
    "    - List of similarities for each word pair, with None for pairs containing OOV words.\n",
    "    \"\"\"\n",
    "    similarities = []\n",
    "    for word1, word2 in word_pairs:\n",
    "        if word1 in word2index and word2 in word2index:\n",
    "            # Retrieve index for each word\n",
    "            word1_idx, word2_idx = word2index[word1], word2index[word2]\n",
    "            \n",
    "            # Average center and outside embeddings for a comprehensive word representation\n",
    "            word1_emb, word2_emb = [\n",
    "                (model.center_embedding(torch.tensor([idx], dtype=torch.long)).squeeze(0) +\n",
    "                 model.outside_embedding(torch.tensor([idx], dtype=torch.long)).squeeze(0)) / 2\n",
    "                for idx in [word1_idx, word2_idx]\n",
    "            ]\n",
    "            \n",
    "            # Convert PyTorch tensors to numpy arrays for cosine similarity calculation\n",
    "            word1_emb_np, word2_emb_np = word1_emb.detach().numpy(), word2_emb.detach().numpy()\n",
    "            \n",
    "            # Compute similarity as 1 minus the cosine distance\n",
    "            similarity = 1 - cosine(word1_emb_np, word2_emb_np)\n",
    "            similarities.append(similarity)\n",
    "        else:\n",
    "            # Append None for word pairs where either word is out of vocabulary\n",
    "            similarities.append(None)\n",
    "            \n",
    "    return similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity_gensim_glove(model, word_pairs):\n",
    "    \"\"\"\n",
    "    Computes cosine similarities for word pairs using a GloVe model loaded with Gensim.\n",
    "    \n",
    "    Args:\n",
    "    - model: Pre-trained GloVe model loaded via Gensim.\n",
    "    - word_pairs: List of tuples containing word pairs for similarity calculation.\n",
    "    \n",
    "    Returns:\n",
    "    - A list of similarities for each word pair, with None for pairs involving OOV words.\n",
    "    \"\"\"\n",
    "    similarities = []\n",
    "    for word1, word2 in word_pairs:\n",
    "        if word1 in model.key_to_index and word2 in model.key_to_index:\n",
    "            similarity = model.similarity(word1, word2)\n",
    "            similarities.append(similarity)\n",
    "        else:\n",
    "            # Append None if either word is not in the model's vocabulary\n",
    "            similarities.append(None)\n",
    "    return similarities\n",
    "\n",
    "def compute_correlation_with_human_judgment(model, dataset):\n",
    "    \"\"\"\n",
    "    Calculates Spearman's rank correlation between model-derived similarities and human judgment scores.\n",
    "    \n",
    "    Args:\n",
    "    - model: The GloVe model loaded with Gensim for which to compute similarities.\n",
    "    - dataset: DataFrame with columns 'word1', 'word2', and 'human_score' indicating human-assigned similarity scores.\n",
    "    \n",
    "    Returns:\n",
    "    - Spearman's rank correlation coefficient between model similarities and human scores.\n",
    "    \"\"\"\n",
    "    word_pairs = list(zip(dataset['word1'], dataset['word2']))\n",
    "    model_similarities = calculate_similarity_gensim_glove(model, word_pairs)\n",
    "    \n",
    "    # Exclude pairs with OOV words to align model and human scores\n",
    "    valid_scores = [(human, model) for human, model in zip(dataset['human_score'], model_similarities) if model is not None]\n",
    "    filtered_human_scores, filtered_model_scores = zip(*valid_scores)\n",
    "    \n",
    "    correlation, _ = spearmanr(filtered_human_scores, filtered_model_scores)\n",
    "    return correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's rank correlation for Skipgram: 0.124\n"
     ]
    }
   ],
   "source": [
    "# Load Skipgram model similarities for word pairs from the DataFrame and compute similarity scores\n",
    "model_similarities_skipgram = calculate_similarity_skipgram_models(loaded_model_Skipgram, list(zip(df['word1'], df['word2'])), word2index_skipgram)\n",
    "# Exclude word pairs with at least one OOV word to ensure valid comparison between model and human scores\n",
    "filtered_human_scores = [human_score for human_score, model_score in zip(df['human_score'], model_similarities_skipgram) if model_score is not None]\n",
    "filtered_model_scores = [model_score for model_score in model_similarities_skipgram if model_score is not None]\n",
    "\n",
    "# Spearman's rank correlation calculation to assess the alignment between model-derived similarities and human judgment\n",
    "correlation, _ = spearmanr(filtered_human_scores, filtered_model_scores)\n",
    "# Output the correlation result, indicating the model's performance in mirroring human semantic judgments\n",
    "print(f\"Spearman's rank correlation for Skipgram: {correlation:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's rank correlation for Skipgram-Neg-Sampling: 0.116\n"
     ]
    }
   ],
   "source": [
    "# Calculate model similarities using the skipgram negative-sampling model\n",
    "model_similarities_skipgram_neg = calculate_similarity_skipgram_models(loaded_model_SkipgramNeg, list(zip(df['word1'], df['word2'])), word2index_SkipgramNeg)\n",
    "\n",
    "# Filter out pairs where at least one word was OOV (Out Of Vocabulary)\n",
    "filtered_human_scores = [human_score for human_score, model_score in zip(df['human_score'], model_similarities_skipgram_neg) if model_score is not None]\n",
    "filtered_model_scores = [model_score for model_score in model_similarities_skipgram_neg if model_score is not None]\n",
    "\n",
    "# Calculate Spearman's rank correlation between human scores and model scores\n",
    "correlation, _ = spearmanr(filtered_human_scores, filtered_model_scores)\n",
    "\n",
    "print(f\"Spearman's rank correlation for Skipgram-Neg-Sampling: {correlation:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's rank correlation for GloVe Scratch: -0.091\n"
     ]
    }
   ],
   "source": [
    "# Calculate model similarities using the GloVe Scratch model\n",
    "model_similarities_Glove_Scratch = calculate_similarity_glove(loaded_model_Glove, list(zip(df['word1'], df['word2'])), word2index_Glove)\n",
    "\n",
    "# Filter out pairs where at least one word was OOV (Out Of Vocabulary)\n",
    "filtered_human_scores = [human_score for human_score, model_score in zip(df['human_score'], model_similarities_Glove_Scratch) if model_score is not None]\n",
    "filtered_model_scores = [model_score for model_score in model_similarities_Glove_Scratch if model_score is not None]\n",
    "\n",
    "# Calculate Spearman's rank correlation between human scores and model scores\n",
    "correlation, _ = spearmanr(filtered_human_scores, filtered_model_scores)\n",
    "\n",
    "# Print the Spearman's rank correlation coefficient for GloVe Scratch model\n",
    "print(f\"Spearman's rank correlation for GloVe Scratch: {correlation:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's rank correlation for GloVe Gensim: 0.602\n"
     ]
    }
   ],
   "source": [
    "# Calculate Spearman's rank correlation using a GloVe Gensim model and a DataFrame\n",
    "correlation = compute_correlation_with_human_judgment(loaded_model_Glove_Gen, df)\n",
    "\n",
    "# Print the Spearman's rank correlation coefficient for GloVe Gensim\n",
    "print(f\"Spearman's rank correlation for GloVe Gensim: {correlation:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model                     | Spearman's Rank Correlation |\n",
      "|---------------------------|-----------------------------|\n",
      "| Skipgram                   |                       0.124 |\n",
      "| Skipgram-Neg-Sampling      |                       0.116 |\n",
      "| GloVe Scratch              |                      -0.091 |\n",
      "| GloVe Gensim               |                       0.602 |\n"
     ]
    }
   ],
   "source": [
    "models = [\"Skipgram\", \"Skipgram-Neg-Sampling\", \"GloVe Scratch\", \"GloVe Gensim\"]\n",
    "correlation_coefficients = [0.124, 0.116, -0.091, 0.602]\n",
    "\n",
    "print(\"| Model                     | Spearman's Rank Correlation |\")\n",
    "print(\"|---------------------------|-----------------------------|\")\n",
    "\n",
    "for model, correlation in zip(models, correlation_coefficients):\n",
    "    print(f\"| {model:<26} | {correlation:>27.3f} |\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spearman's rank correlation coefficient is a measure of the strength and direction of association between two ranked variables. In the context of word embedding models:\n",
    "\n",
    "Skipgram: This model achieved a Spearman's rank correlation coefficient of 0.124.\n",
    "\n",
    "Skipgram-Neg-Sampling: This model achieved a Spearman's rank correlation coefficient of 0.116.\n",
    "\n",
    "GloVe Scratch: This model achieved a Spearman's rank correlation coefficient of -0.091.\n",
    "\n",
    "GloVe Gensim: This model achieved a Spearman's rank correlation coefficient of 0.602.\n",
    "\n",
    "These coefficients indicate the degree of similarity between the similarity scores predicted by the models and the human-rated similarity scores for word pairs. A coefficient closer to 1 indicates a strong positive correlation, while a coefficient closer to -1 indicates a \n",
    "strong negative correlation. A coefficient close to 0 suggests a weak correlation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Computer-programming",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
